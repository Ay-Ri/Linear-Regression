{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.What is Simple Linear Regression?\n",
        "\n",
        "Ans: Simple Linear Regression is a statistical method used to model the relationship between two variables:\n",
        "\n",
        "Independent Variable (X): Also called the predictor or input.\n",
        "\n",
        "Dependent Variable (Y): Also called the response or output.\n",
        "\n",
        "It assumes that the relationship between these two variables is linear, meaning it can be represented with a straight line.\n"
      ],
      "metadata": {
        "id": "PV3i8uFOQ6X0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Ans: 1. Linearity\n",
        "There should be a linear relationship between the independent variable (X) and the dependent variable (Y).\n",
        "\n",
        "In other words, changes in X result in proportional changes in Y.\n",
        "\n",
        " Check using: Scatter plot of X vs Y or residual plot.\n",
        "\n",
        " 2. Independence of Errors\n",
        "The residuals (errors) should be independent of each other.\n",
        "\n",
        "Especially important in time series data where observations can be dependent on time.\n",
        "\n",
        " Check using: Durbin-Watson test for autocorrelation.\n",
        "\n",
        " 3. Homoscedasticity (Constant Variance of Errors)\n",
        "The variance of residuals should remain constant across all values of X.\n",
        "\n",
        "If the spread of residuals increases or decreases with X, the assumption is violated.\n",
        "\n",
        " Check using: Residual plot (should show random scatter).\n",
        "\n",
        " 4. Normality of Errors\n",
        "The residuals should be normally distributed.\n",
        "\n",
        "This is mainly important for hypothesis testing and confidence intervals.\n",
        "\n",
        " Check using: Histogram or Q-Q plot of residuals.\n",
        "\n",
        " 5. No Multicollinearity (Note: Applies to multiple regression)\n",
        "For simple linear regression (with only one X), this is not applicable.\n",
        "\n",
        "For multiple linear regression: independent variables should not be highly correlated with each other.\n"
      ],
      "metadata": {
        "id": "ptxYqOqDQ6Ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "Ans: It tells you how much Y changes for a one-unit increase in X.\n",
        "\n",
        "In other words, it indicates the rate of change of the dependent variable (Y) with respect to the independent variable (X).\n",
        "\n",
        "Interpretation:\n",
        "If m>0: As X increases, Y increases → positive relationship\n",
        "\n",
        "If m<0: As X increases, Y decreases → negative relationship\n",
        "\n",
        "If m=0: Y does not change with X → no linear relationship"
      ],
      "metadata": {
        "id": "9kqi5zsoQ6R4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        "Ans: c is the value of Y when the independent variable X=0.\n",
        "\n",
        "It tells you where the line crosses the Y-axis on a graph.\n",
        "\n",
        "Interpretation:\n",
        "It provides a baseline value of the dependent variable before any influence of\n",
        "X.\n",
        "\n",
        "Important for understanding the starting point of the linear relationship.\n"
      ],
      "metadata": {
        "id": "49VE1hOLQ6PO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "Ans: In Simple Linear Regression, the slope m (also called β1) represents how much the dependent variable Y changes for a one-unit increase in the independent variable X."
      ],
      "metadata": {
        "id": "9i7xI2dhQ6MZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "Ans: The Least Squares Method is used to find the best-fitting line through a set of data points by minimizing the total error between the predicted values and actual values.\n",
        "\n",
        "In Simple Terms:\n",
        "It finds the line Y=mX+c that makes the sum of squared differences between actual and predicted values as small as possible.\n",
        "\n"
      ],
      "metadata": {
        "id": "wfsuch32Q6Cu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        "Ans: The coefficient of determination, denoted as R2, measures the proportion of variance in the dependent variable (Y) that is explained by the independent variable (X) in the linear regression model.\n"
      ],
      "metadata": {
        "id": "hoPRVtryTFcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regression?\n",
        "\n",
        "Ans: Multiple Linear Regression (MLR) is an extension of Simple Linear Regression that models the relationship between one dependent variable (Y) and two or more independent variables (X₁, X₂, ..., Xₙ)."
      ],
      "metadata": {
        "id": "PrV1u9GsTGQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "Ans: In simple linear function there are 1 independent variable, simple to easy and visualize, Examines relationships between one factor and outcome, overfitting risk is lower and easier to interpret.\n",
        "\n",
        "In multiple linear function there are two or more independent variables, more complex, needs higher-dimensional visualization, examines how multiple factors influence the outcome, overfiting risk is higher, interpretation is slightly harder due to multiple variables."
      ],
      "metadata": {
        "id": "Jpj0_GTJTGNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "Ans: 1. Linearity\n",
        "The relationship between the dependent variable and each independent variable is linear.\n",
        "\n",
        "2. Independence of Errors\n",
        "Residuals (errors) should be independent of each other.\n",
        "\n",
        "Especially important in time series data.\n",
        "\n",
        "3. Homoscedasticity (Constant Variance of Errors)\n",
        "The variance of residuals should be constant across all levels of the independent variables.\n",
        "\n",
        "4. Normality of Residuals\n",
        "Residuals should be normally distributed for valid hypothesis tests and confidence intervals.\n",
        "\n",
        "5. No Multicollinearity\n",
        "Independent variables should not be too highly correlated with each other.\n",
        "\n",
        "Multicollinearity makes it hard to separate the effects of individual predictors.\n",
        "\n",
        "6. No Autocorrelation (for time series data)\n",
        "Residuals should not be correlated over time (i.e., no patterns in the errors).\n"
      ],
      "metadata": {
        "id": "EG7yCkQyTGLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Ans: Heteroscedasticity refers to a condition in regression analysis where the variance of the residuals (errors) is not constant across all levels of the independent variables.\n",
        "\n",
        "In Simple Terms:\n",
        "In a well-behaved regression model, the spread of the residuals should be roughly the same no matter what the value of the independent variable(s) — this is called homoscedasticity.\n",
        "\n",
        "When that spread increases or decreases systematically, we call it heteroscedasticity."
      ],
      "metadata": {
        "id": "NUESxzBKTGI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "Ans: Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, meaning they convey similar information about the variance in the dependent variable.\n",
        "\n",
        "Improve the Model:\n",
        "1. Remove Highly Correlated Predictors\n",
        "Drop one of the variables with high correlation (e.g., if X1 and X2 have 95% correlation, remove one)\n",
        "\n",
        "2. Combine Predictors\n",
        "Use Principal Component Analysis (PCA) or Factor Analysis to combine correlated features into uncorrelated components\n",
        "\n",
        "3. Use Regularization Techniques\n",
        "Apply models that can handle multicollinearity:\n",
        "\n",
        "Ridge Regression (L2 regularization): shrinks coefficients of correlated variables\n",
        "\n",
        "Lasso Regression (L1 regularization): can eliminate some coefficients entirely\n",
        "\n",
        "4. Centering (Mean Normalization)\n",
        "Subtract the mean from each predictor to reduce non-essential collinearity, especially in interaction terms.\n",
        "\n",
        "5. Domain Knowledge\n",
        "Use business or domain knowledge to prioritize features that make more sense and drop redundant ones"
      ],
      "metadata": {
        "id": "2m2sSb8DTGGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "Ans: 1. One-Hot Encoding (Dummy Variables)\n",
        "Each category is turned into a new binary (0/1) column.\n",
        "\n",
        "Example:\n",
        "For a column Color with values: Red, Blue, Green\n",
        "\n",
        "2. Label Encoding\n",
        "Each category is assigned a unique integer.\n",
        "\n",
        "Example:\n",
        "[Red, Blue, Green] → [0, 1, 2]\n",
        "\n",
        "3. Ordinal Encoding\n",
        "Used when categories have a natural order (e.g., Low < Medium < High).\n",
        "\n",
        "Example:\n",
        "[Low, Medium, High] → [1, 2, 3]\n",
        "\n",
        "4. Binary Encoding\n",
        "Converts categories into binary code and splits digits into separate columns.\n",
        "More compact than one-hot for high-cardinality data.\n",
        "\n",
        "5. Frequency or Count Encoding\n",
        "Replace each category with its frequency (number of times it appears in the dataset).\n",
        "\n",
        "Example:\n",
        "{'Red': 10, 'Blue': 50, 'Green': 20}"
      ],
      "metadata": {
        "id": "BnoWqrXBTGDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Ans: Interaction terms are used in multiple linear regression to capture the combined effect of two or more independent variables on the dependent variable that is not explained by their individual effects alone."
      ],
      "metadata": {
        "id": "3fAqOAqJTGBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "Ans: In regression analysis, the intercept is a critical part of the regression equation. However, its interpretation changes depending on whether you're using simple or multiple linear regression. Let’s explore the differences in a clear and descriptive manner."
      ],
      "metadata": {
        "id": "3z3U8AEBTF-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "Ans: ✅ What is the Slope?\n",
        "In regression analysis, the slope (denoted as β1 in simple linear regression) is a coefficient that represents the rate of change in the dependent variable (Y) for a one-unit increase in the independent variable (X)."
      ],
      "metadata": {
        "id": "AxZxl3SGTF8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "Ans:\n",
        "In a regression model, the intercept (often denoted as β0) is a fundamental component that gives context and a starting point for the relationship between the independent variables and the dependent variable."
      ],
      "metadata": {
        "id": "XhQeXsIeTF5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "Ans: While R² (coefficient of determination) is a commonly used metric to assess the goodness-of-fit of a regression model, relying on it alone can be misleading.\n",
        "\n",
        "1. R² Only Measures Fit, Not Accuracy\n",
        "R² tells you how well the model explains the variation in the dependent variable.\n",
        "\n",
        "It does not measure predictive accuracy on new or unseen data.\n",
        "\n",
        "Example: A model with a high R² on training data may perform poorly on test data due to overfitting.\n",
        "\n",
        "2. R² Increases with More Variables\n",
        "In Multiple Linear Regression, adding more predictors—even irrelevant ones—always increases R² (or keeps it the same).\n",
        "\n",
        "This can lead to overfitting and false confidence in the model's performance.\n",
        "\n",
        "That’s why we often use Adjusted R², which penalizes unnecessary variables.\n",
        "\n",
        "3. R² Doesn’t Reveal Model Bias or Errors\n",
        "R² says nothing about residual patterns, bias, or assumption violations (e.g., heteroscedasticity, autocorrelation).\n",
        "\n",
        "You could have a high R² and still violate basic assumptions of regression.\n",
        "\n",
        "4. Not Useful for Comparing Different Types of Models\n",
        "R² is not always comparable across non-linear models or models with different structures.\n",
        "\n",
        "Example: You can’t use R² to directly compare a linear regression with a decision tree or neural network.\n",
        "\n",
        "5. Doesn’t Capture Practical Significance\n",
        "A model may have a statistically high R² but the actual changes in the dependent variable might be too small to matter in practice.\n",
        "\n",
        "High R² doesn’t always mean the model is useful or actionable.\n",
        "\n",
        "6. Can Be Misleading in Nonlinear Relationships\n",
        "R² assumes a linear relationship between variables.\n",
        "\n",
        "In non-linear models, R² might be low even if the model fits the data well."
      ],
      "metadata": {
        "id": "sv0lM3mGTF2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "Ans: The standard error of a regression coefficient measures the average variability or uncertainty in that coefficient estimate due to sampling error. It tells us how precise our estimate of the coefficient is.\n",
        "\n",
        "If the Standard Error is Large:\n",
        "A large standard error means that the coefficient is not estimated precisely — it may vary a lot across different samples.\n",
        "\n",
        "How to Interpret a Large Standard Error:\n",
        "The coefficient may not be statistically significant\n",
        "\n",
        "A large standard error can result in a high p-value, meaning we cannot be confident that the predictor actually has an effect on the response variable.\n",
        "\n",
        "There is high uncertainty about the true effect\n",
        "\n",
        "The regression model is unsure about the true size or direction of the coefficient.\n",
        "\n",
        "Confidence intervals are wide\n",
        "\n",
        "The 95% confidence interval for the coefficient will span a large range, possibly including zero → again suggesting low reliability.\n",
        "\n"
      ],
      "metadata": {
        "id": "KgQwdd1-TF0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "Ans: Heteroscedasticity occurs when the variance of residuals (errors) in a regression model changes across the values of the independent variables.\n",
        "\n",
        "In a well-behaved model, the residuals should have constant variance → called homoscedasticity.\n",
        "\n",
        "When this is not the case, and error variance increases or decreases with predictors or fitted values, it's heteroscedasticity."
      ],
      "metadata": {
        "id": "SpHYbLO9TFxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "Ans: R² (Coefficient of Determination)\n",
        "Measures how well the model explains the variance in the dependent variable.\n",
        "\n",
        "Always increases (or stays the same) when more predictors are added, even if they are irrelevant.\n",
        "\n",
        "Adjusted R²\n",
        "Adjusts R² for the number of predictors in the model.\n",
        "\n",
        "Penalizes the model for including variables that don’t improve model performance.\n",
        "\n",
        "Can decrease if you add unnecessary predictors."
      ],
      "metadata": {
        "id": "8sB-eFASTFvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Ans: Scaling refers to transforming variables so that they share a common scale, typically by:\n",
        "\n",
        "Standardization: converting to z-scores → mean = 0, std. deviation = 1\n",
        "\n",
        "Normalization: rescaling to a range, usually [0, 1]"
      ],
      "metadata": {
        "id": "V0Kpc1MWTFnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression?\n",
        "\n",
        "Ans: Polynomial Regression is a type of regression analysis in which the relationship between the independent variable X and the dependent variable\n",
        "Y is modeled as an nth-degree polynomial.\n",
        "\n",
        "It is a special case of linear regression, where we model non-linear relationships using linear combinations of powers of the input variable."
      ],
      "metadata": {
        "id": "PRL1FeaNTFkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression?\n",
        "\n",
        "Ans: Linear Regression is ideal when the data points follow a straight-line pattern.\n",
        "\n",
        "Polynomial Regression fits better when data shows curved or non-linear trends.\n",
        "\n",
        "Use Linear Regression if your data shows a straight-line pattern.\n",
        "\n",
        "Use Polynomial Regression if the relationship is curved and linear models underfit."
      ],
      "metadata": {
        "id": "ImHsWM31TFhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used?\n",
        "\n",
        "Ans: Polynomial regression is used when the relationship between the independent variable(s) and the dependent variable is non-linear, but can be modeled as a polynomial function. It allows us to fit a curved line to data using powers of the input variable(s)."
      ],
      "metadata": {
        "id": "sPAdTvQVjORn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression?\n",
        "\n",
        "Ans: Polynomial Regression models the relationship between a dependent variable\n",
        "Y and an independent variable X as an nth-degree polynomial."
      ],
      "metadata": {
        "id": "4ZkBxqCsjoWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Ans: Yes, polynomial regression can be extended to multiple variables — this is known as multivariate polynomial regression (or polynomial multiple regression)"
      ],
      "metadata": {
        "id": "_GnkBTQLjoS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression?\n",
        "\n",
        "Ans: While Polynomial Regression is a powerful tool for modeling non-linear relationships, it comes with several important limitations that should be considered before applying it.\n",
        "\n",
        " 1. Overfitting\n",
        "Higher-degree polynomials can fit the training data too closely, capturing noise instead of the true pattern.\n",
        "\n",
        "This leads to poor generalization on new or unseen data.\n",
        "\n",
        " Example: A 10th-degree polynomial may perfectly fit 10 data points, but perform terribly on the 11th point.\n",
        "\n",
        "2. Extrapolation Problems\n",
        "Polynomial functions can behave wildly outside the range of the training data.\n",
        "\n",
        "Predictions beyond the data range can become unrealistic and unstable, especially with high degrees.\n",
        "\n",
        " Example: A cubic model might suddenly shoot up or down at the edges of your data.\n",
        "\n",
        "3. Computational Cost & Complexity\n",
        "The number of features increases rapidly with the polynomial degree and number of input variables.\n",
        "\n",
        "This leads to:\n",
        "\n",
        "Higher memory and processing requirements\n",
        "\n",
        "Longer training time\n",
        "\n",
        "Complex models that are hard to inter"
      ],
      "metadata": {
        "id": "g3B9u94QjoPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "Ans: Choosing the right degree for a polynomial regression model is crucial to avoid underfitting or overfitting. Here are some key methods to evaluate and select the best-fitting polynomial degree:\n",
        "\n",
        "1. Visual Inspection (Plotting the Fit)\n",
        "Plot the data points and fitted curve for various degrees.\n",
        "\n",
        "Helps detect:\n",
        "\n",
        "Underfitting (too simple/linear)\n",
        "\n",
        "Overfitting (too wiggly/complex)\n",
        "\n",
        " Useful but subjective.\n",
        "\n",
        "2. Train-Test Split (Holdout Method)\n",
        "Split the data into training and testing sets.\n",
        "\n",
        "Fit polynomial models of different degrees on the training set.\n",
        "\n",
        "Compare test set performance using error metrics like:\n",
        "\n",
        "MSE (Mean Squared Error)\n",
        "\n",
        "RMSE (Root Mean Squared Error)\n",
        "\n",
        "MAE (Mean Absolute Error)\n",
        "\n",
        "Choose the degree with the lowest test error.\n",
        "\n",
        "3. Cross-Validation (e.g., k-Fold CV)\n",
        "More reliable than train-test split.\n",
        "\n",
        "Divide the dataset into k folds, train on k−1 folds, validate on the remaining fold, and repeat.\n",
        "\n",
        "Compute the average validation error for each degree.\n",
        "\n",
        "Use cross-validated error to choose the degree with best generalization.\n",
        "\n",
        "4. Adjusted R²\n",
        "Unlike R², Adjusted R² penalizes for adding unnecessary polynomial terms.\n",
        "\n",
        "Use it to evaluate whether adding more degrees genuinely improves model fit.\n",
        "\n",
        " Higher Adjusted R² indicates better model fit with fewer unnecessary terms."
      ],
      "metadata": {
        "id": "-4OeZG43joLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression?\n",
        "\n",
        "Ans: Visualization is crucial in polynomial regression because it helps you understand, evaluate, and communicate how well the model captures the relationship between the independent and dependent variables — especially when that relationship is non-linear.\n",
        "\n",
        "1. Understand the Relationship Between Variables\n",
        "Polynomial regression is designed to model curved patterns.\n",
        "\n",
        "Visualization helps you see the shape of the relationship (e.g., U-shape, S-curve) and judge whether a polynomial model is appropriate.\n",
        "\n",
        " You can quickly spot if the data has a non-linear trend that linear regression would miss.\n",
        "\n",
        "2. Detect Underfitting and Overfitting\n",
        "Fit Type\tWhat You See in the Plot\n",
        "Underfitting\tModel is too simple → curve doesn’t follow the data well\n",
        "Good fit\tCurve follows the general shape of the data\n",
        "Overfitting\tCurve is too wiggly → fits noise, not trend\n",
        "\n",
        " Visual clues are often clearer than numeric metrics.\n",
        "\n",
        "3. Choose the Right Degree of the Polynomial\n",
        "By plotting curves for different degrees (e.g., 1st, 2nd, 3rd), you can visually assess:\n",
        "\n",
        "Which model balances bias and variance\n",
        "\n",
        "Which degree best fits the trend without overfitting\n",
        "\n",
        " Validation curves and residual plots also help."
      ],
      "metadata": {
        "id": "tJiP-kEwjoHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is polynomial regression implemented in Python?\n",
        "\n",
        "Ans: Polynomial regression in Python is most commonly implemented using Scikit-learn. It combines PolynomialFeatures to generate polynomial terms and LinearRegression to fit the model.\n",
        "\n",
        "1. import libraries\n",
        "\n",
        "2.Create sample data\n",
        "\n",
        "3. Build the polynomial regression model\n",
        "\n",
        "4. Makes prediction and plots"
      ],
      "metadata": {
        "id": "-Rm0IJBtjoEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RrAs1X4zjoAj"
      }
    }
  ]
}